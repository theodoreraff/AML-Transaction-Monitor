{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T01:41:06.432202Z",
     "start_time": "2025-07-18T01:41:06.430466Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# PORTFOLIO PROJECT: AML ANOMALY DETECTION PROTOTYPE\n",
    "# BY: Theodorus\n",
    "# VERSION: 2.1 (Error fix in report generation)\n",
    "# =============================================================================="
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:41:24.700321Z",
     "start_time": "2025-07-18T01:41:24.698017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Please run 'pip install pandas numpy' in your terminal if you haven't already\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os"
   ],
   "id": "ec8a02217d5e5c38",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:44:09.729789Z",
     "start_time": "2025-07-18T01:44:09.715502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# MISSION #2: PREPARE THE \"CRIME SCENE DATA\" (GENERATE FICTITIOUS DATA)\n",
    "# ==============================================================================\n",
    "# In this section, we will create a fake transaction dataset and save it to a .csv file\n",
    "# inside the 'data/' directory. This keeps our project organized.\n",
    "\n",
    "print(\"Starting Mission #2: Creating the fictitious transaction dataset...\")\n",
    "\n",
    "def generate_transaction_data(num_records=1000, output_folder='data'):\n",
    "    \"\"\"\n",
    "    Generates fictitious transaction data and saves it to a CSV file.\n",
    "    Intentionally injects anomalous data for detection purposes.\n",
    "    Saves the output to the specified folder.\n",
    "    \"\"\"\n",
    "    # --- Create the data directory if it doesn't exist ---\n",
    "    if not os.path.exists(output_folder):\n",
    "        print(f\"Directory '{output_folder}' not found. Creating it now.\")\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    output_path = os.path.join(output_folder, 'transactions.csv')\n",
    "\n",
    "    data = []\n",
    "    user_ids = [f'user_{i}' for i in range(50)]\n",
    "    base_date = datetime.now()\n",
    "\n",
    "    # Generate normal data\n",
    "    for i in range(num_records):\n",
    "        data.append({\n",
    "            'transaction_id': f'txn_{1000+i}',\n",
    "            'user_id': random.choice(user_ids),\n",
    "            'amount': round(random.uniform(10000, 5000000), 2),\n",
    "            'timestamp': base_date - timedelta(days=random.randint(1, 30), hours=random.randint(0, 23)),\n",
    "            'location_country': 'ID'\n",
    "        })\n",
    "\n",
    "    # --- Inject Anomalies ---\n",
    "\n",
    "    # Anomaly 1: High-Frequency Transactions\n",
    "    print(\"Injecting Anomaly #1: High-Frequency Transactions...\")\n",
    "    anomaly_user_1 = 'user_123'\n",
    "    for i in range(6):\n",
    "        data.append({\n",
    "            'transaction_id': f'txn_hf_{i}',\n",
    "            'user_id': anomaly_user_1,\n",
    "            'amount': round(random.uniform(50000, 200000), 2),\n",
    "            'timestamp': base_date - timedelta(minutes=i),\n",
    "            'location_country': 'ID'\n",
    "        })\n",
    "\n",
    "    # Anomaly 2: High-Amount Transaction\n",
    "    print(\"Injecting Anomaly #2: High-Amount Transaction...\")\n",
    "    data.append({\n",
    "        'transaction_id': 'txn_ha_1',\n",
    "        'user_id': 'user_456',\n",
    "        'amount': 150000000.00,\n",
    "        'timestamp': base_date - timedelta(days=2),\n",
    "        'location_country': 'ID'\n",
    "    })\n",
    "\n",
    "    # Anomaly 3: Impossible Travel\n",
    "    print(\"Injecting Anomaly #3: Impossible Travel...\")\n",
    "    anomaly_user_3 = 'user_789'\n",
    "    data.append({\n",
    "        'transaction_id': 'txn_it_1',\n",
    "        'user_id': anomaly_user_3,\n",
    "        'amount': 750000.00,\n",
    "        'timestamp': base_date - timedelta(hours=5),\n",
    "        'location_country': 'ID'\n",
    "    })\n",
    "    data.append({\n",
    "        'transaction_id': 'txn_it_2',\n",
    "        'user_id': anomaly_user_3,\n",
    "        'amount': 1200000.00,\n",
    "        'timestamp': base_date - timedelta(hours=4, minutes=30),\n",
    "        'location_country': 'RU'\n",
    "    })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # Save to CSV file inside the 'data' folder\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Dataset '{output_path}' created successfully.\")\n",
    "    return df\n",
    "\n",
    "# Run the function to create the data\n",
    "df = generate_transaction_data()\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ],
   "id": "7bb8d0487a490274",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Mission #2: Creating the fictitious transaction dataset...\n",
      "Injecting Anomaly #1: High-Frequency Transactions...\n",
      "Injecting Anomaly #2: High-Amount Transaction...\n",
      "Injecting Anomaly #3: Impossible Travel...\n",
      "Dataset 'data/transactions.csv' created successfully.\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:45:08.709700Z",
     "start_time": "2025-07-18T01:45:08.698915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# MISSION #3: BUILD THE \"DETECTION ENGINE\"\n",
    "# ==============================================================================\n",
    "# Here we will load the data from the 'data/' folder, apply our detection rules,\n",
    "# and report the findings.\n",
    "\n",
    "print(\"Starting Mission #3: Building the anomaly detection engine...\")\n",
    "\n",
    "# --- 1. Load & Clean Data ---\n",
    "print(\"\\nStep 1: Loading and exploring the data...\")\n",
    "data_path = 'data/transactions.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "# Convert timestamp column to datetime data type\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "print(f\"Data loaded successfully from '{data_path}'. Here are the first 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n"
   ],
   "id": "4c22585ab31a8253",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Mission #3: Building the anomaly detection engine...\n",
      "\n",
      "Step 1: Loading and exploring the data...\n",
      "Data loaded successfully from 'data/transactions.csv'. Here are the first 5 rows:\n",
      "  transaction_id  user_id      amount                  timestamp  \\\n",
      "0       txn_1000  user_44  3134370.21 2025-06-17 13:44:09.720961   \n",
      "1       txn_1001  user_18  1849730.66 2025-07-06 10:44:09.720961   \n",
      "2       txn_1002  user_41  2413923.93 2025-07-12 11:44:09.720961   \n",
      "3       txn_1003  user_30  3035717.93 2025-06-26 02:44:09.720961   \n",
      "4       txn_1004  user_23  1450715.91 2025-07-13 08:44:09.720961   \n",
      "\n",
      "  location_country  \n",
      "0               ID  \n",
      "1               ID  \n",
      "2               ID  \n",
      "3               ID  \n",
      "4               ID  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1009 entries, 0 to 1008\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   transaction_id    1009 non-null   object        \n",
      " 1   user_id           1009 non-null   object        \n",
      " 2   amount            1009 non-null   float64       \n",
      " 3   timestamp         1009 non-null   datetime64[ns]\n",
      " 4   location_country  1009 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(3)\n",
      "memory usage: 39.5+ KB\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:45:22.853597Z",
     "start_time": "2025-07-18T01:45:22.841760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2. Implement Rule-Based Detection Logic ---\n",
    "print(\"\\nStep 2: Implementing detection rules...\")\n",
    "\n",
    "suspicious_transactions = []\n",
    "\n",
    "# Rule #1: High-Frequency Detection\n",
    "print(\"\\n   -> Applying Rule #1: High-Frequency Detection...\")\n",
    "df_sorted_hf = df.sort_values(by=['user_id', 'timestamp'])\n",
    "grouped = df_sorted_hf.groupby('user_id')['timestamp'].apply(list)\n",
    "for user, timestamps in grouped.items():\n",
    "    if len(timestamps) >= 6:\n",
    "        for i in range(len(timestamps) - 5):\n",
    "            if (timestamps[i+5] - timestamps[i]) < timedelta(minutes=10):\n",
    "                suspicious_indices = df[\n",
    "                    (df['user_id'] == user) &\n",
    "                    (df['timestamp'] >= timestamps[i]) &\n",
    "                    (df['timestamp'] <= timestamps[i+5])\n",
    "                ].index\n",
    "                for idx in suspicious_indices:\n",
    "                    suspicious_transactions.append({'index': idx, 'reason': 'Suspicious - High Frequency'})\n"
   ],
   "id": "4cfba10702deaa72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Implementing detection rules...\n",
      "\n",
      "   -> Applying Rule #1: High-Frequency Detection...\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:45:30.990568Z",
     "start_time": "2025-07-18T01:45:30.986704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rule #2: High-Amount Detection\n",
    "print(\"   -> Applying Rule #2: High-Amount Detection...\")\n",
    "high_amount_threshold = 50_000_000\n",
    "high_amount_indices = df[df['amount'] > high_amount_threshold].index\n",
    "for idx in high_amount_indices:\n",
    "    suspicious_transactions.append({'index': idx, 'reason': 'Suspicious - High Amount'})\n"
   ],
   "id": "820e4cb147a2060c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Applying Rule #2: High-Amount Detection...\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:45:40.964192Z",
     "start_time": "2025-07-18T01:45:40.956550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rule #3: Impossible Travel Detection\n",
    "print(\"   -> Applying Rule #3: Impossible Travel Detection...\")\n",
    "df_sorted_it = df.sort_values(by=['user_id', 'timestamp']).copy()\n",
    "df_sorted_it['prev_location'] = df_sorted_it.groupby('user_id')['location_country'].shift(1)\n",
    "df_sorted_it['prev_timestamp'] = df_sorted_it.groupby('user_id')['timestamp'].shift(1)\n",
    "impossible_travel_df = df_sorted_it[\n",
    "    (df_sorted_it['location_country'] != df_sorted_it['prev_location']) &\n",
    "    (df_sorted_it['prev_location'].notna()) &\n",
    "    ((df_sorted_it['timestamp'] - df_sorted_it['prev_timestamp']) < timedelta(hours=1))\n",
    "]\n",
    "for idx in impossible_travel_df.index:\n",
    "    suspicious_transactions.append({'index': idx, 'reason': 'Suspicious - Impossible Travel'})\n"
   ],
   "id": "e728d8e0e13a6f36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Applying Rule #3: Impossible Travel Detection...\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:45:48.410647Z",
     "start_time": "2025-07-18T01:45:48.402523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3. Generate Final Report ---\n",
    "print(\"\\nStep 3: Creating the investigation report...\")\n",
    "if not suspicious_transactions:\n",
    "    print(\"No suspicious transactions were found.\")\n",
    "else:\n",
    "    # Create a DataFrame from the list of suspicious transaction dictionaries\n",
    "    suspicious_df = pd.DataFrame(suspicious_transactions)\n",
    "    # Remove duplicate flags for the same transaction and set the original index as the new index\n",
    "    suspicious_df = suspicious_df.drop_duplicates(subset='index').set_index('index')\n",
    "\n",
    "    # The join is done on the index, so the 'on' parameter is not needed.\n",
    "    report_df = df.join(suspicious_df)\n",
    "\n",
    "    # Filter out the rows that were not flagged (where 'reason' is NaN)\n",
    "    # and rename the column for clarity.\n",
    "    report_df = report_df[report_df['reason'].notna()].rename(columns={'reason': 'detection_reason'})\n",
    "\n",
    "    print(\"\\n\" + \"=\"*20 + \" PRELIMINARY INVESTIGATION REPORT \" + \"=\"*20)\n",
    "    print(f\"Found {len(report_df)} transactions flagged as anomalies:\")\n",
    "    # Display the final report, ensuring the index is not shown for cleaner output\n",
    "    print(report_df[['transaction_id', 'user_id', 'amount', 'timestamp', 'location_country', 'detection_reason']].to_string())\n",
    "    print(\"=\"*68)\n",
    "\n",
    "print(\"\\nAnalysis complete. The preliminary investigation report has been generated.\")\n"
   ],
   "id": "2d1e840806487444",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Creating the investigation report...\n",
      "\n",
      "==================== PRELIMINARY INVESTIGATION REPORT ====================\n",
      "Found 8 transactions flagged as anomalies:\n",
      "     transaction_id   user_id        amount                  timestamp location_country                detection_reason\n",
      "1000       txn_hf_0  user_123  1.320920e+05 2025-07-18 08:44:09.720961               ID     Suspicious - High Frequency\n",
      "1001       txn_hf_1  user_123  1.680767e+05 2025-07-18 08:43:09.720961               ID     Suspicious - High Frequency\n",
      "1002       txn_hf_2  user_123  7.559840e+04 2025-07-18 08:42:09.720961               ID     Suspicious - High Frequency\n",
      "1003       txn_hf_3  user_123  7.771510e+04 2025-07-18 08:41:09.720961               ID     Suspicious - High Frequency\n",
      "1004       txn_hf_4  user_123  1.471520e+05 2025-07-18 08:40:09.720961               ID     Suspicious - High Frequency\n",
      "1005       txn_hf_5  user_123  1.007550e+05 2025-07-18 08:39:09.720961               ID     Suspicious - High Frequency\n",
      "1006       txn_ha_1  user_456  1.500000e+08 2025-07-16 08:44:09.720961               ID        Suspicious - High Amount\n",
      "1008       txn_it_2  user_789  1.200000e+06 2025-07-18 04:14:09.720961               RU  Suspicious - Impossible Travel\n",
      "====================================================================\n",
      "\n",
      "Analysis complete. The preliminary investigation report has been generated.\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
